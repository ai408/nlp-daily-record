D:\Python310\python.exe L:\20230825_NLP工程化公众号\nlp-engineering\20230916_Llama2-Chinese\tools\merge_llama_with_lora.py --base_model L:/20230713_HuggingFaceModel/20230903_Llama2/Llama-2-7b-hf --lora_model ./checkpoint-500 --output_type pth --output_dir ./merge_llama_with_lora_pt_output

===================================BUG REPORT===================================
Welcome to bitsandbytes. For bug reports, please run

python -m bitsandbytes

 and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues
================================================================================
bin D:\Python310\lib\site-packages\bitsandbytes\libbitsandbytes_cuda118.dll
CUDA SETUP: CUDA runtime path found: C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v11.8\bin\cudart64_110.dll
CUDA SETUP: Highest compute capability among GPUs detected: 8.6
CUDA SETUP: Detected CUDA version 118
CUDA SETUP: Loading binary D:\Python310\lib\site-packages\bitsandbytes\libbitsandbytes_cuda118.dll...
[2023-10-02 14:24:11,637] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
NOTE: Redirects are currently not supported in Windows or MacOs.
Base model: L:/20230713_HuggingFaceModel/20230903_Llama2/Llama-2-7b-hf
LoRA model(s) ['./checkpoint-500']:
Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.34s/it]
LlamaForCausalLM(
  (model): LlamaModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-31): 32 x LlamaDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation()
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
)
Peft version: 0.6.0.dev0
Loading LoRA for 7B model
Loading LoRA ./checkpoint-500
Merging with merge_and_unload...
Saving to pth format...
Saving shard 1 of 1 into ./merge_llama_with_lora_pt_output/consolidated.00.pth

Process finished with exit code 0